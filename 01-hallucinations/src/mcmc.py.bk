import tensorflow as tf
import numpy as np
import sys

from resnet import *
from utils import *

def mcmc(seq0,ref,nsteps,nsave,beta,CHK):

    config = tf.ConfigProto(
        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.92)
    )


    # decide whether to optimize for a given topology
    # or to generate brand new ones
    use_bkgrd = True
    if len(ref['dist'].shape) == 2:
        use_bkgrd = False

    L = len(seq0)

    traj = []

    #
    # network
    #
    with tf.Graph().as_default():

        # inputs
        with tf.name_scope('input'):

            ncol = tf.placeholder(dtype=tf.int32, shape=())
            msa = tf.placeholder(dtype=tf.uint8, shape=(None,None))

            if use_bkgrd == True:
                bd = tf.placeholder(dtype=tf.float32, shape=(None,None,None))
                bo = tf.placeholder(dtype=tf.float32, shape=(None,None,None))
                bt = tf.placeholder(dtype=tf.float32, shape=(None,None,None))
                bp = tf.placeholder(dtype=tf.float32, shape=(None,None,None))

            else:
                dist = tf.placeholder(dtype=tf.uint8, shape=(None,None))
                omega = tf.placeholder(dtype=tf.uint8, shape=(None,None))
                theta = tf.placeholder(dtype=tf.uint8, shape=(None,None))
                phi = tf.placeholder(dtype=tf.uint8, shape=(None,None))

            is_train = tf.placeholder(tf.bool, name='is_train')

        # convert inputs to 1-hot
        msa1hot  = tf.one_hot(msa, 21, dtype=tf.float32)

        if use_bkgrd == False:
            dist1hot = tf.one_hot(dist, 37, dtype=tf.float32)
            omega1hot = tf.one_hot(omega, 25, dtype=tf.float32)
            theta1hot = tf.one_hot(theta, 25, dtype=tf.float32)
            phi1hot = tf.one_hot(phi, 13, dtype=tf.float32)

        # collect features
        w = reweight(msa1hot, 0.8)
        f1d_seq = msa1hot[0,:,:20]
        f1d_pssm = msa2pssm(msa1hot, w)
        f1d = tf.concat(values=[f1d_seq, f1d_pssm], axis=1)
        f1d = tf.expand_dims(f1d, axis=0)
        f1d = tf.reshape(f1d, [1,ncol,42])
        f2d_dca = tf.zeros([ncol,ncol,442], tf.float32)
        f2d_dca = tf.expand_dims(f2d_dca, axis=0)
        f2d = tf.concat([tf.tile(f1d[:,:,None,:], [1,1,ncol,1]),
                        tf.tile(f1d[:,None,:,:], [1,ncol,1,1]),
                        f2d_dca], axis=-1)
        f2d = tf.reshape(f2d, [1,ncol,ncol,442+2*42])

        # resnet
        logits_dist,logits_omega,logits_theta,logits_phi = resnet(f2d,61,is_train)

        # losses
        if use_bkgrd == False:

            # optimize for target topology
            loss_theta = -tf.math.reduce_mean(logits_theta[0]*theta1hot)
            loss_phi = -tf.math.reduce_mean(logits_phi[0]*phi1hot)
            loss_dist = -tf.math.reduce_mean(logits_dist[0]*dist1hot)
            loss_omega = -tf.math.reduce_mean(logits_omega[0]*omega1hot)

        else:

            # generate random topology
            prob_dist = tf.nn.softmax(logits_dist)[0]
            loss_dist = -tf.math.reduce_mean(tf.math.reduce_sum(prob_dist*tf.math.log(prob_dist/bd),axis=-1))

            prob_omega = tf.nn.softmax(logits_omega)[0]
            loss_omega = -tf.math.reduce_mean(tf.math.reduce_sum(prob_omega*tf.math.log(prob_omega/bo),axis=-1))

            prob_theta = tf.nn.softmax(logits_theta)[0]
            loss_theta = -tf.math.reduce_mean(tf.math.reduce_sum(prob_theta*tf.math.log(prob_theta/bt),axis=-1))

            prob_phi = tf.nn.softmax(logits_phi)[0]
            loss_phi = -tf.math.reduce_mean(tf.math.reduce_sum(prob_phi*tf.math.log(prob_phi/bp),axis=-1))

        # total loss
        loss = loss_dist + loss_omega + loss_theta + loss_phi


        saver = tf.train.Saver()
        with tf.Session(config=config) as sess:

            saver.restore(sess, CHK)

            # initialize with initial sequence
            seq = aa2idx(seq0).copy().reshape([1,L])
            E = 999.9

            # mcmc steps
            for i in range(nsteps):

                # random mutation at random position
                idx = np.random.randint(L)
                a = np.random.randint(20)
                seq_curr = np.copy(seq)
                seq_curr[0,idx] = a

                # probe effect of mutation
                if use_bkgrd == True:
                    E_curr = sess.run(loss, feed_dict = {
                        msa : seq_curr, ncol : L,
                        bd : ref['dist'], bo : ref['omega'], bt : ref['theta'], bp : ref['phi'],
                        is_train : 0})

                else:
                    E_curr = sess.run(loss, feed_dict = {
                        msa : seq_curr, ncol : L,
                        dist : ref['dist'], omega : ref['omega'], theta : ref['theta'], phi : ref['phi'],
                        is_train : 0})

                # Metropolis criterion
                if E_curr < E:
                    seq = np.copy(seq_curr)
                    E = E_curr
                else:
                    if np.exp((E-E_curr)*beta) > np.random.uniform():
                        seq = np.copy(seq_curr)
                        E = E_curr

                if i%nsave==0:
                    aa = idx2aa(seq[0])
                    print("%8d %s %.6f"%(i, aa, E))
                    sys.stdout.flush()
                    traj.append([i,aa,E])

    return traj
